{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as spark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = SparkSession.builder.getOrCreate()\n",
    "\n",
    "iris = sc.read.csv(\n",
    "    r\"G:\\Meu Drive\\Data Science\\Dados\\Classificação\\Iris\\Iris.csv\",\n",
    "    header = True\n",
    ")\n",
    "\n",
    "iris.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|        Species|count|\n",
      "+---------------+-----+\n",
      "| Iris-virginica|   50|\n",
      "|    Iris-setosa|   50|\n",
      "|Iris-versicolor|   50|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.groupBy(\"Species\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elaboração de uma Pipeline de Aprendizado de Máquina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+---------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|        Species|\n",
      "+---+-------------+------------+-------------+------------+---------------+\n",
      "| 51|         51.0|        51.0|         51.0|        51.0|Iris-versicolor|\n",
      "| 52|         52.0|        52.0|         52.0|        52.0|Iris-versicolor|\n",
      "| 53|         53.0|        53.0|         53.0|        53.0|Iris-versicolor|\n",
      "+---+-------------+------------+-------------+------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Altera o tipo de dado de uma coluna, aceita como argumento os tipos integer, double, string, etc.\n",
    "iris = iris.withColumn(\"SepalLengthCm\", iris.Id.cast(\"double\"))\n",
    "iris = iris.withColumn(\"SepalWidthCm\", iris.Id.cast(\"double\"))\n",
    "iris = iris.withColumn(\"PetalLengthCm\", iris.Id.cast(\"double\"))\n",
    "iris = iris.withColumn(\"PetalWidthCm\", iris.Id.cast(\"double\"))\n",
    "\n",
    "# Para exemplificação queremos apenas duas espécies de Iris para criar um classificador de binário:\n",
    "iris = iris.filter((iris.Species == \"Iris-virginica\") | (iris.Species == \"Iris-versicolor\"))\n",
    "iris.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, SepalLengthCm: double, SepalWidthCm: double, PetalLengthCm: double, PetalWidthCm: double, Species: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+---------------+-------------+-------------+--------------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|        Species|Species_Index| Species_Fact|            features|\n",
      "+---+-------------+------------+-------------+------------+---------------+-------------+-------------+--------------------+\n",
      "| 52|         52.0|        52.0|         52.0|        52.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[52.0,52.0,52.0,5...|\n",
      "| 56|         56.0|        56.0|         56.0|        56.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[56.0,56.0,56.0,5...|\n",
      "| 57|         57.0|        57.0|         57.0|        57.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[57.0,57.0,57.0,5...|\n",
      "| 58|         58.0|        58.0|         58.0|        58.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[58.0,58.0,58.0,5...|\n",
      "| 60|         60.0|        60.0|         60.0|        60.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[60.0,60.0,60.0,6...|\n",
      "| 63|         63.0|        63.0|         63.0|        63.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[63.0,63.0,63.0,6...|\n",
      "| 65|         65.0|        65.0|         65.0|        65.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[65.0,65.0,65.0,6...|\n",
      "| 66|         66.0|        66.0|         66.0|        66.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[66.0,66.0,66.0,6...|\n",
      "| 67|         67.0|        67.0|         67.0|        67.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[67.0,67.0,67.0,6...|\n",
      "| 68|         68.0|        68.0|         68.0|        68.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[68.0,68.0,68.0,6...|\n",
      "| 70|         70.0|        70.0|         70.0|        70.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[70.0,70.0,70.0,7...|\n",
      "| 71|         71.0|        71.0|         71.0|        71.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[71.0,71.0,71.0,7...|\n",
      "| 72|         72.0|        72.0|         72.0|        72.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[72.0,72.0,72.0,7...|\n",
      "| 73|         73.0|        73.0|         73.0|        73.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[73.0,73.0,73.0,7...|\n",
      "| 75|         75.0|        75.0|         75.0|        75.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[75.0,75.0,75.0,7...|\n",
      "| 78|         78.0|        78.0|         78.0|        78.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[78.0,78.0,78.0,7...|\n",
      "| 82|         82.0|        82.0|         82.0|        82.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[82.0,82.0,82.0,8...|\n",
      "| 83|         83.0|        83.0|         83.0|        83.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[83.0,83.0,83.0,8...|\n",
      "| 84|         84.0|        84.0|         84.0|        84.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[84.0,84.0,84.0,8...|\n",
      "| 85|         85.0|        85.0|         85.0|        85.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[85.0,85.0,85.0,8...|\n",
      "+---+-------------+------------+-------------+------------+---------------+-------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "label does not exist. Available: Id, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm, Species, Species_Index, Species_Fact, features, CrossValidator_a660479720dc_rand",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 32\u001b[0m\n\u001b[0;32m     24\u001b[0m grid \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mbuild()\n\u001b[0;32m     26\u001b[0m cv \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39mCrossValidator(\n\u001b[0;32m     27\u001b[0m     estimator \u001b[39m=\u001b[39m lr,\n\u001b[0;32m     28\u001b[0m     estimatorParamMaps \u001b[39m=\u001b[39m grid,\n\u001b[0;32m     29\u001b[0m     evaluator \u001b[39m=\u001b[39m evaluator\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m models \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39;49mfit(training)\n\u001b[0;32m     33\u001b[0m best_lr \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mbestModel\n\u001b[0;32m     35\u001b[0m test_results \u001b[39m=\u001b[39m best_lr\u001b[39m.\u001b[39mtransform(test)\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    841\u001b[0m train \u001b[39m=\u001b[39m datasets[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcache()\n\u001b[0;32m    843\u001b[0m tasks \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\n\u001b[0;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[0;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[0;32m    846\u001b[0m )\n\u001b[1;32m--> 847\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[0;32m    848\u001b[0m     metrics_all[i][j] \u001b[39m=\u001b[39m metric\n\u001b[0;32m    849\u001b[0m     \u001b[39mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[1;32m--> 873\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    123\u001b[0m job, i, func, args, kwds \u001b[39m=\u001b[39m task\n\u001b[0;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     result \u001b[39m=\u001b[39m (\u001b[39mTrue\u001b[39;00m, func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[0;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m wrap_exception \u001b[39mand\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    841\u001b[0m train \u001b[39m=\u001b[39m datasets[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcache()\n\u001b[0;32m    843\u001b[0m tasks \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\n\u001b[0;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[0;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[0;32m    846\u001b[0m )\n\u001b[1;32m--> 847\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[0;32m    848\u001b[0m     metrics_all[i][j] \u001b[39m=\u001b[39m metric\n\u001b[0;32m    849\u001b[0m     \u001b[39mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\util.py:337\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    336\u001b[0m SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39msetLocalProperties(properties)\n\u001b[1;32m--> 337\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[1;34m()\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msingleTask\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m, Transformer]:\n\u001b[1;32m--> 113\u001b[0m     index, model \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(modelIter)\n\u001b[0;32m    114\u001b[0m     \u001b[39m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[39m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39m#  all nested stages and evaluators\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     metric \u001b[39m=\u001b[39m eva\u001b[39m.\u001b[39mevaluate(model\u001b[39m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo models remaining.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m index, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfitSingleModel(index)\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfitSingleModel\u001b[39m(index: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m M:\n\u001b[1;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator\u001b[39m.\u001b[39;49mfit(dataset, paramMaps[index])\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(params, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    202\u001b[0m     \u001b[39mif\u001b[39;00m params:\n\u001b[1;32m--> 203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy(params)\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(dataset)\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[0;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\zheag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: label does not exist. Available: Id, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm, Species, Species_Index, Species_Fact, features, CrossValidator_a660479720dc_rand"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import pyspark.ml.evaluation as evals\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "species_indexer = StringIndexer(inputCol = \"Species\", outputCol = \"Species_Index\")\n",
    "species_encoder = OneHotEncoder(inputCol = \"Species_Index\", outputCol = \"Species_Fact\")\n",
    "vec_assembler = VectorAssembler(inputCols = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], outputCol = \"features\")\n",
    "\n",
    "iris_pipe = Pipeline(stages = [species_indexer, species_encoder, vec_assembler])\n",
    "\n",
    "piped_data = iris_pipe.fit(iris).transform(iris)\n",
    "training, test = piped_data.randomSplit([0.6, 0.4])\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName = \"areaUnderROC\")\n",
    "\n",
    "grid = tune.ParamGridBuilder()\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, 0.1, 0.01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "\n",
    "grid = grid.build()\n",
    "\n",
    "cv = tune.CrossValidator(\n",
    "    estimator = lr,\n",
    "    estimatorParamMaps = grid,\n",
    "    evaluator = evaluator\n",
    ")\n",
    "\n",
    "models = cv.fit(training)\n",
    "best_lr = models.bestModel\n",
    "\n",
    "test_results = best_lr.transform(test)\n",
    "\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+---------------+-------------+-------------+--------------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|        Species|Species_Index| Species_Fact|            features|\n",
      "+---+-------------+------------+-------------+------------+---------------+-------------+-------------+--------------------+\n",
      "| 51|         51.0|        51.0|         51.0|        51.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[51.0,51.0,51.0,5...|\n",
      "| 53|         53.0|        53.0|         53.0|        53.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[53.0,53.0,53.0,5...|\n",
      "| 54|         54.0|        54.0|         54.0|        54.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[54.0,54.0,54.0,5...|\n",
      "| 57|         57.0|        57.0|         57.0|        57.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[57.0,57.0,57.0,5...|\n",
      "| 58|         58.0|        58.0|         58.0|        58.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[58.0,58.0,58.0,5...|\n",
      "| 59|         59.0|        59.0|         59.0|        59.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[59.0,59.0,59.0,5...|\n",
      "| 60|         60.0|        60.0|         60.0|        60.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[60.0,60.0,60.0,6...|\n",
      "| 61|         61.0|        61.0|         61.0|        61.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[61.0,61.0,61.0,6...|\n",
      "| 62|         62.0|        62.0|         62.0|        62.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[62.0,62.0,62.0,6...|\n",
      "| 63|         63.0|        63.0|         63.0|        63.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[63.0,63.0,63.0,6...|\n",
      "| 64|         64.0|        64.0|         64.0|        64.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[64.0,64.0,64.0,6...|\n",
      "| 66|         66.0|        66.0|         66.0|        66.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[66.0,66.0,66.0,6...|\n",
      "| 68|         68.0|        68.0|         68.0|        68.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[68.0,68.0,68.0,6...|\n",
      "| 71|         71.0|        71.0|         71.0|        71.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[71.0,71.0,71.0,7...|\n",
      "| 75|         75.0|        75.0|         75.0|        75.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[75.0,75.0,75.0,7...|\n",
      "| 76|         76.0|        76.0|         76.0|        76.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[76.0,76.0,76.0,7...|\n",
      "| 77|         77.0|        77.0|         77.0|        77.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[77.0,77.0,77.0,7...|\n",
      "| 78|         78.0|        78.0|         78.0|        78.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[78.0,78.0,78.0,7...|\n",
      "| 80|         80.0|        80.0|         80.0|        80.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[80.0,80.0,80.0,8...|\n",
      "| 83|         83.0|        83.0|         83.0|        83.0|Iris-versicolor|          0.0|(1,[0],[1.0])|[83.0,83.0,83.0,8...|\n",
      "+---+-------------+------------+-------------+------------+---------------+-------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=EMACjF6eCU4&ab_channel=Stack retomar no tempo 17min \n",
    "\n",
    "https://www.youtube.com/watch?v=8esz7IWSbMM&ab_channel=AprenderDados%7CBernardoCambruzzi assistir\n",
    "\n",
    "https://community.cloud.databricks.com/?o=4566189763914988#notebook/3855061166299265 notebook no databricks\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLIHpLBNsiHE3Zmdc8Hc8H8n8TVpmrb-fp playlist de engenharia de dados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
