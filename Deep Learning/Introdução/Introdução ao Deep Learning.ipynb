{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mãos à Obra: Aprendizado de Máquina com Scikit-Learn, Keras & TensorFlow. Retomar da página 221"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron é uma das arquiteturas mais simples de RNNAs. Tem como base o chamado unidade lógica de limiar (TLU).\n",
    "\n",
    "<p align=\"center\"> <img src=\"Perceptron.png\" width=\"500\"> </p>\n",
    "\n",
    "- Suas entradas e saídas são números;\n",
    "- Cada conexão de entrada está associada a um peso;\n",
    "- a TLU calcula uma soma ponderada de suas entradas, $z=w_1x_1+\\cdots+w_nx_n$;\n",
    "- Aplicamos uma função degrau para a soma obtida no passo anterior;\n",
    "- Temos como resultado um classificador $h_w(x)=\\text{degrau}(z)$.\n",
    "\n",
    "Podemos usar diferentes funções degrau para Perceptrons, sendo as mais populares:\n",
    "\n",
    "$$\\text{heaviside}(z) = \\left\\{\\begin{matrix}\n",
    "0 \\text{ se } z<0\\\\ \n",
    "1 \\text{ se } z\\geq 0\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "e \n",
    "\n",
    "$$\n",
    "\\text{sgn}(z) = \\left\\{\\begin{matrix}\n",
    "-1 &\\text{ se } z<0\\\\ \n",
    "0 &\\text{ se } z= 0\\\\\n",
    "1 &\\text{ se } z>0\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "Em resumo, podemos calcular as saídas de um Perceptron através da seguinte equação:\n",
    "\n",
    "$$h_{W,b}(X) = \\phi(XW+b)$$\n",
    "\n",
    "onde:\n",
    "\n",
    "- $W$ é a matriz de pesos das conexões de $X$ (exceto o neurônio de viés);\n",
    "- $b$ é o vetor de viés, contendo todos os pesos de conexão entre o neurônio de viés e os neurônios artificiais;\n",
    "- $\\phi$ é a função de ativação.\n",
    "\n",
    "Para atualizarmos os pesos do Perceptron podemos utilizar a seguinte regra de aprendizado:\n",
    "\n",
    "$$ w_{i,j}^{(\\text{próximo passo})} = w_{i,j} + \\eta (y_j - \\hat{y}_j)x_i$$\n",
    "\n",
    "- $w_{i,j}$ é o peso da conexão entre o i-ésimo neurônio de entrada e o j-ésimo neurônio de saída;\n",
    "- $\\eta$ é a taxa de aprendizado.\n",
    "\n",
    "> **Teorema de convergência do Perceptron:** Perceptron é um algoritmo cuja fronteira de decisão é linear. Dessa forma, o algoritmo não consegue prever padrões complexos dos dados. Apesar disso, caso os dados de treinamento sejam linearmente separáveis, é possível demonstrar que o algoritmo convergirá para esta solução separável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf_Per = Perceptron()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
