{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, relembremos que o estimador de Ridge tem como função a ser minimizada:\n",
    "\n",
    "$$\n",
    "r^{(\\lambda)}(\\beta) = (y-X\\beta)^T(y-X\\beta) + \\lambda \\beta^T \\beta.\n",
    "$$\n",
    "\n",
    "Para isto, observemos que\n",
    "\n",
    "$$\n",
    "r^{(\\lambda)}(\\beta) = y^Ty-y^T X\\beta -(X\\beta)^Ty + (X\\beta)^T X\\beta + \\lambda \\beta^T \\beta\n",
    "$$\n",
    "\n",
    "como $(X\\beta)^T = \\beta^T X^T$ segue que\n",
    "\n",
    "$$\n",
    "r^{(\\lambda)}(\\beta) = y^Ty-y^T X\\beta -\\beta^T X^T y + \\beta^T X^T X\\beta + \\lambda \\beta^T \\beta\n",
    "$$\n",
    "\n",
    "Como $y^T X\\beta$ é um escalar (matriz 1x1), temos que $y^T X\\beta = (y^T X\\beta)^T = \\beta^T X^T y$. Daí, derivando em relação a $\\beta$ e igualando a zero, temos\n",
    "\n",
    "$$-2 y^T X + 2X^T X\\beta +2 \\lambda \\beta= 0$$\n",
    "\n",
    "Logo,\n",
    "\n",
    "$$X^T X \\beta + \\lambda \\beta = y^T X \\Rightarrow  \\beta = (X^T X + \\lambda I)^{-1} y^T X $$\n",
    "\n",
    "Consequentemente,\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{Ridge}}(\\lambda) = (I + (X^TX)^{-1} \\lambda)^{-1} (X^TX)^{-1} X^T y = (I + (X^TX)^{-1} \\lambda)^{-1} \\beta_{\\text{MQ}}\n",
    "$$\n",
    "\n",
    "Portanto, \n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{Ridge}}(\\lambda) = (I + (X^TX)^{-1} \\lambda)^{-1} \\beta_{\\text{MQ}}\n",
    "$$\n",
    "\n",
    "> **Observação:** por questões de rigorosidade, ainda é necessário mostrar que $\\hat{\\beta}_{\\text{Ridge}}$ é mínimo global. Para isto precisávamos derivar a equação inicial duas vezes em relação a $\\beta$ e observar o sinal da derivada. Para isto, basta ver que\n",
    ">\n",
    "> $$\\frac{\\partial^2}{\\partial \\beta^2} r^{(\\lambda)}(\\beta) = X^T X+ \\lambda I $$\n",
    ">\n",
    "> Para isto, nos apoiamos no resultado de que se $X$ é uma matriz de posto completa, então $X^TX$ é positiva definida. Neste caso, $X$ possuí posto completo como uma das suposições de modelos lineares e, consequentemente $X^T X$ é uma matriz de posto completo, consequentemente $X^TX$ é positiva definida (assim como $X^TX+ \\lambda I$, pois $\\lambda>0$) e segue que é de fato $\\hat{\\beta}_{\\text{Ridge}}(\\lambda)$ ponto de mínimo global. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A segunda parte é bem simples, basta ver que\n",
    "\n",
    "$$\\mathbb{E}[\\hat{\\beta}_{\\text{Ridge}}(\\lambda)] = \\mathbb{E}[(I + (X^TX)^{-1} \\lambda)^{-1} \\beta_{\\text{MQ}}] = (I + (X^TX)^{-1} \\lambda)^{-1} \\mathbb{E}[\\beta_{\\text{MQ}}] = (I + (X^TX)^{-1} \\lambda)^{-1} \\beta \\neq \\beta$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
